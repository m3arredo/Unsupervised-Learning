{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "46a30c86-283d-4390-af9a-cc7205844c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk.corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from scipy.cluster.hierarchy import fcluster, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "99681a4a-dadf-4f9f-b169-20d3699f0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english') + list(string.punctuation) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "11c45b50-5655-407d-8805-f2e6cabd5e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ed97c14d-3541-47a9-a9fa-2d78c2590c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2377ec75-fdd6-472a-a621-0e1f537892c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [i.lower() for i in words] #first lc since stop is case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b233bb52-b27b-4fd3-a12a-c7d52dbc7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [i for i in words if i.isalpha() and i not in stop and len(i)> 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "335aeffb-8db0-43e1-a69e-6866997feab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508631"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea9d35-3631-4b7f-a7e3-3d9c627342ce",
   "metadata": {},
   "source": [
    "#### Words and frequency (Same as before just another method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3aeffb47-b840-4cc9-9941-c24b799a5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(words)\n",
    "words1 = [] \n",
    "total = {}\n",
    "for i in range(1,N-1):\n",
    "    w = words[i]\n",
    "    if w not in words1:\n",
    "        words1.append(w)\n",
    "        total[w] = 0\n",
    "    total[w] = total[w]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "13805a48-1913-43c8-8848-7a033182fe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts = sorted(total.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_dict = dict(sorted_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d54e3102-fa26-45fe-8de8-a13f29cba8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 3292),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292),\n",
       " ('man', 1207),\n",
       " ('even', 1170),\n",
       " ('made', 1125),\n",
       " ('also', 1069),\n",
       " ('many', 1030),\n",
       " ('must', 1013),\n",
       " ('af', 996),\n",
       " ('back', 966),\n",
       " ('years', 950),\n",
       " ('much', 937)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted_dict.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e840135-d108-4d63-9c72-98766d31290f",
   "metadata": {},
   "source": [
    "#### Top Words in vocab and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6c2aa48f-21ea-4ff5-b455-025cc04343fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for w,count in list(sorted_dict.items())[:5000]]\n",
    "contxt_words = [w for w,count in list(sorted_dict.items())[:1000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4525e7-315f-44b6-bf71-64cb1ef0d9fa",
   "metadata": {},
   "source": [
    "#### Window of four words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "733c77ea-d597-4a27-b9bd-4a23bf7e4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts(window_size=2):\n",
    "    counts = {}\n",
    "    J = [ j for j in range(-window_size,0)] + [j for j in range(1,window_size+1) ]\n",
    "    for w0 in vocabulary :\n",
    "        counts[w0] = {}\n",
    "    for i in range(window_size, N - (window_size) ):\n",
    "        w0 = words[i]\n",
    "        if w0 in vocabulary:  #any word in vocab - w0\n",
    "            for j in J:\n",
    "                w = words[i+j]\n",
    "                if w in contxt_words: #any word in contxt - w\n",
    "                    if w not in counts[w0].keys():\n",
    "                        counts[w0][w] = 1\n",
    "                    else:\n",
    "                        counts[w0][w] = counts[w0][w] + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f70be713-9a7f-4bb5-8b32-08ae4728714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability over the context w0 , counts[w0][w] / sum of all counts[w0][]\n",
    "def get_co_occurrence_dict(counts):\n",
    "    probs = {}\n",
    "    for w0 in counts.keys():\n",
    "        sum = 0\n",
    "        for w in counts[w0].keys():\n",
    "            sum = sum+counts[w0][w]\n",
    "        if sum >0:\n",
    "            probs[w0] = {}\n",
    "            for w in counts[w0].keys():\n",
    "                probs[w0][w] = float(counts[w0][w]) / float(sum)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f441769e-2ba2-4461-9b7b-e8bd664474c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency of different contxt words,  sum all counts[][w] / sum all counts [][]\n",
    "def get_contxt_distr(counts):\n",
    "    counts_contxt = {}\n",
    "    sum_contxt = 0 \n",
    "    contxt_frequency = {}\n",
    "    for w in contxt_words:\n",
    "        counts_contxt[w] = 0\n",
    "    for w0 in counts.keys():\n",
    "        for w in counts[w0].keys():\n",
    "            counts_contxt[w] = counts_contxt[w] + counts[w0][w]\n",
    "            sum_contxt = sum_contxt + counts[w0][w]\n",
    "    for w in contxt_words :\n",
    "        contxt_frequency[w] = float (counts_contxt[w]) / float(sum_contxt)\n",
    "    return contxt_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b0dd4e60-296f-4077-b6b6-7ae14518f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = get_counts(2)\n",
    "probability = get_co_occurrence_dict(counts)\n",
    "contxt_frequency = get_contxt_distr(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562acca-2441-4978-baba-4f593d2d64e6",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a8d994b9-da4e-4ff8-b738-5408bf280975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PMI pointwise mutual information\n",
    "n_vocab = len(vocabulary)\n",
    "n_contxt = len(contxt_words)\n",
    "pmi = np.zeros((n_vocab, n_contxt))\n",
    "for i in range(0,n_vocab):\n",
    "    w0 = vocabulary[i]\n",
    "    for w in probability[w0].keys():\n",
    "        j = contxt_words.index(w)\n",
    "        pmi[i,j] = max(0.0, np.log(probability[w0][w]) - np.log(contxt_frequency[w]) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ace6c-f651-4151-8f40-b7c93cdbd8db",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c91faa87-82b0-4071-8e8d-c684daab8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "vecs = pca.fit_transform(pmi) #fitting into 100 dimens\n",
    "for i in range(0,n_vocab):\n",
    "        vecs[i] = vecs[i] / np.linalg.norm(vecs[i]) #normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b05e8-3b6b-446a-bc94-e576319bbd22",
   "metadata": {},
   "source": [
    "I implemented a word embedding by constructing a co-occurrence matrix, selected the most used words for the vocabulary(5000) and context words(1000), and calculated probabilities and distributions. Using PCA, I reduced the dimensionality of the Pointwise Mutual Information matrix to obtain a 100-dimensional word embedding, capturing semantic relationships between words. The resulting vectors represent contextual similarities in a more compact form, offering a meaningful representation of word semantics in the given corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf024f8-2a26-4e7a-8cf3-dcdfb2a8355b",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5fbf9ed3-749f-46de-a170-1ad0fd124213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_NN(w):\n",
    "    if not(w in vocabulary):\n",
    "        print(\"Uknown Words\")\n",
    "        return\n",
    "    v = vecs[vocabulary.index(w)]\n",
    "    neighbor = 0 \n",
    "    curr_dist = 1 - np.dot(v, vecs[0]) / (np.linalg.norm(v) * np.linalg.norm(vecs[0]) ) \n",
    "    for i in range(1, n_vocab):\n",
    "        dist = 1 - np.dot(v, vecs[i]) / (np.linalg.norm(v) * np.linalg.norm(vecs[i]) ) \n",
    "        if (dist < curr_dist) and dist>0.0 :\n",
    "            neighbor = i\n",
    "            curr_dist = dist\n",
    "    return vocabulary[neighbor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ce901e5a-3ac9-4dce-a184-4c21ddfc8330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['milligrams', 'blonde', 'teach', 'art', 'study', 'roof', 'cents', 'pilot', 'baker', 'cady', 'extend', 'gross', 'used', 'seemed', 'glad', 'agreed', 'fair', 'category', 'classes', 'editor', 'mechanisms', 'specialists', 'heard', 'remainder', 'grant']\n"
     ]
    }
   ],
   "source": [
    "selected_vocab = random.sample(vocabulary,25)\n",
    "print(selected_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "50c10319-8cb7-4cc7-948e-4553b892794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "milligrams dairy\n",
      "blonde hair\n",
      "teach learn\n",
      "art modern\n",
      "study present\n",
      "roof corner\n",
      "cents pound\n",
      "pilot expanded\n",
      "baker quiney\n",
      "cady companion\n",
      "extend scope\n",
      "gross returns\n",
      "used use\n",
      "seemed felt\n",
      "glad see\n",
      "agreed obliged\n",
      "fair care\n",
      "category occurrence\n",
      "classes families\n",
      "editor signed\n",
      "mechanisms phases\n",
      "specialists surplus\n",
      "heard hear\n",
      "remainder crops\n",
      "grant state\n"
     ]
    }
   ],
   "source": [
    "for w in selected_vocab:\n",
    "    nn = word_NN(w)\n",
    "    print(w, nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ce57f-b1bd-466d-a4d0-189503790bd7",
   "metadata": {},
   "source": [
    "The provided word pairs and their nearest neighbors suggest that the word embedding captures meaningful semantic relationships for some terms, but the quality of associations varies, indicating potential areas for improvement in the model or parameters. Further evaluation and adjustment may enhance the embedding's ability to represent coherent and contextually relevant word similarities.\n",
    "\n",
    "        - \"blonde hair\" and \"milligrams dairy\" may represent associations between related concepts.\n",
    "        -\"study present\" and \"teach learn\" suggest a connection between learning-related terms.\n",
    "Some pairs appear less related or could be ambiguous:\n",
    "\n",
    "        - \"roof corner\" and \"cents pound\" may lack clear semantic connections.\n",
    "        - \"agreed obliged\" could be contextually dependent, and the association may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061aee7f-8003-49c0-9f94-160a119fe4d8",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f5f3de1b-4934-402b-b9d3-4dad8e65cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: closely, apply, experienced, advanced, agree\n",
      "Cluster 2: movement, firm, attitude, race, attempt\n",
      "Cluster 3: like, man, way, little, still\n",
      "Cluster 4: side, line, feet, center, outside\n",
      "Cluster 5: killed, hole, birds, putting, engine\n",
      "Cluster 6: described, test, names, procedure, treated\n",
      "Cluster 7: af, points, image, plane, fixed\n",
      "Cluster 8: jobs, joined, automobile, congregation, includes\n",
      "Cluster 9: money, tax, pay, amount, paid\n",
      "Cluster 10: believed, die, grow, baby, mine\n",
      "Cluster 11: politics, speaking, minds, version, centuries\n",
      "Cluster 12: may, made, must, however, part\n",
      "Cluster 13: silent, luck, lucy, angry, lovely\n",
      "Cluster 14: public, government, social, national, local\n",
      "Cluster 15: post, bank, forest, located, roads\n",
      "Cluster 16: pressure, range, showed, normal, volume\n",
      "Cluster 17: mere, dangerous, destroyed, inevitably, fail\n",
      "Cluster 18: systems, designed, techniques, materials, permit\n",
      "Cluster 19: time, last, year, week, month\n",
      "Cluster 20: along, car, office, town, road\n",
      "Cluster 21: states, act, section, claim, authority\n",
      "Cluster 22: world, united, war, south, west\n",
      "Cluster 23: lie, detective, strike, sharply, spoken\n",
      "Cluster 24: rate, increase, costs, higher, increased\n",
      "Cluster 25: measure, balance, domestic, achieve, extremely\n",
      "Cluster 26: hot, sun, filled, winter, fresh\n",
      "Cluster 27: removed, remarkable, warning, simultaneously, navy\n",
      "Cluster 28: see, get, come, go, right\n",
      "Cluster 29: parallel, coating, row, ocean, drill\n",
      "Cluster 30: york, providence, virginia, massachusetts, grand\n",
      "Cluster 31: game, team, louis, san, league\n",
      "Cluster 32: even, much, well, make, work\n",
      "Cluster 33: personnel, organizations, armed, agency, conducted\n",
      "Cluster 34: day, night, morning, hour, summer\n",
      "Cluster 35: per, cost, total, million, cent\n",
      "Cluster 36: mentioned, actions, greek, feelings, identity\n",
      "Cluster 37: nodded, stairs, shut, watson, stepped\n",
      "Cluster 38: role, dance, sex, career, achievement\n",
      "Cluster 39: men, life, old, young, children\n",
      "Cluster 40: history, word, music, art, english\n",
      "Cluster 41: court, allowed, agreed, master, relief\n",
      "Cluster 42: eyes, white, black, red, dark\n",
      "Cluster 43: school, college, university, students, class\n",
      "Cluster 44: many, people, american, group, among\n",
      "Cluster 45: tissue, staining, filling, presentation, variation\n",
      "Cluster 46: said, yes, maybe, oh, sir\n",
      "Cluster 47: struck, huge, noticed, approached, switch\n",
      "Cluster 48: funds, vocational, projects, opportunities, boards\n",
      "Cluster 49: miles, river, park, houses, valley\n",
      "Cluster 50: back, came, house, around, went\n",
      "Cluster 51: fields, properly, hopes, sports, weakness\n",
      "Cluster 52: also, since, used, less, system\n",
      "Cluster 53: pride, mystery, release, occasional, characters\n",
      "Cluster 54: personality, artery, warfare, pressures, resulted\n",
      "Cluster 55: island, rhode, boats, towns, comment\n",
      "Cluster 56: john, william, george, manager, king\n",
      "Cluster 57: century, book, wrote, read, received\n",
      "Cluster 58: considered, performance, opportunity, quality, obviously\n",
      "Cluster 59: human, experience, personal, nature, natural\n",
      "Cluster 60: pictures, concrete, weapon, focus, magic\n",
      "Cluster 61: machine, signs, stream, uniform, impressive\n",
      "Cluster 62: use, program, business, development, special\n",
      "Cluster 63: would, could, good, never, know\n",
      "Cluster 64: away, put, hand, head, toward\n",
      "Cluster 65: independence, liberal, negroes, democrats, regions\n",
      "Cluster 66: la, mantle, weak, et, exciting\n",
      "Cluster 67: figure, top, cut, paper, piece\n",
      "Cluster 68: new, service, company, central, army\n",
      "Cluster 69: point, form, change, lines, process\n",
      "Cluster 70: two, years, three, later, days\n",
      "Cluster 71: party, democratic, campaign, leader, khrushchev\n",
      "Cluster 72: occasion, orders, appearance, notes, views\n",
      "Cluster 73: daily, feed, cattle, chemical, animals\n",
      "Cluster 74: adopted, suffered, involves, combined, determining\n",
      "Cluster 75: glass, sea, bottle, round, beat\n",
      "Cluster 76: great, small, number, large, within\n",
      "Cluster 77: state, city, president, members, board\n",
      "Cluster 78: jury, occupied, suggestion, sentence, openly\n",
      "Cluster 79: resolution, begins, ages, medium, driven\n",
      "Cluster 80: friend, ones, charles, mary, patient\n",
      "Cluster 81: address, cell, text, contained, formula\n",
      "Cluster 82: sections, properties, unusual, stages, intelligence\n",
      "Cluster 83: french, wine, dream, songs, painting\n",
      "Cluster 84: god, love, spirit, fear, born\n",
      "Cluster 85: trial, file, hearing, ordered, page\n",
      "Cluster 86: mouth, fell, pain, neck, pulled\n",
      "Cluster 87: warren, humor, alexander, terrible, demanded\n",
      "Cluster 88: high, water, light, body, air\n",
      "Cluster 89: nuclear, goes, begin, learn, forced\n",
      "Cluster 90: willing, save, advice, firmly, hoped\n",
      "Cluster 91: one, played, divided, runs, dozen\n",
      "Cluster 92: sam, connection, animal, imagination, objects\n",
      "Cluster 93: first, another, found, every, took\n",
      "Cluster 94: ship, fighting, tree, pull, occasionally\n",
      "Cluster 95: unity, bitter, contrary, tragedy, everywhere\n",
      "Cluster 96: hanover, parker, surprised, liked, blind\n",
      "Cluster 97: earth, effects, radiation, possibility, shelter\n",
      "Cluster 98: metal, steel, arranged, liquor, iron\n",
      "Cluster 99: index, practices, detailed, gradually, tables\n",
      "Cluster 100: equipment, stock, plant, supply, oil\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 100\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_assignments = kmeans.fit_predict(vecs)\n",
    "\n",
    "clustered_vocab = {w: cluster_assignments[i] for i, w in enumerate(vocabulary)}\n",
    "\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_words = [w for w, cluster in clustered_vocab.items() if cluster == cluster_num]\n",
    "    print(f\"Cluster {cluster_num + 1}: {', '.join(cluster_words[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293d7f9-67e2-4e0a-8e8b-78cab0cfe05e",
   "metadata": {},
   "source": [
    "I decided to use K-means clustering on word embeddings as it organizes words into clusters based on semantic similarity, simplifying the representation of high-dimensional spaces. The resulting clusters provide an interpretable structure, aiding in the understanding of semantic relationships among words. K-means is computationally efficient and scalable, making it suitable for large datasets and facilitating quantitative evaluation of the clustering results.\n",
    "\n",
    "    The best clusters are : \n",
    "     - Cluster 14: public, government, social, national, local\n",
    "     - Cluster 40: history, word, music, art, english\n",
    "     - Cluster 43: school, college, university, students, class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96600afd-1b90-4ccb-ad5d-98b6f81de688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
